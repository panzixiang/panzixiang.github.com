---
layout: post
title: "Week 3 Parameter estimation"
description: ""
category: 
tags: []
---
{% include JB/setup %}

## GDELT Column data aggregation and transformation

We are separating the treatment of event metadata and event impact to generate a map of metadata and and real number quantifying the impact of the event.

### Metadata aggregation

We are aggregating the data rows first by country and then by ActorType1Code and deriving a time series out of them. Each Type will be a regressor and each country will be a vector of types to be fed into VAR. The countries we choose will be the country pair of the exchange rate (for now) and we are assigning different codes to international organizations (UN, EU, NATO etc) to that they have an entity representation.

#### Sample results

USA_20050101 (Country and date)

|Column                | Positive impact  | Negative impact |
|:---------------------|:-----------------|:----------------|
|BUS (business)        | +50              |-20              |    
|GOV (government)      | +10              |-15              |
|...                   |                  |                 |

...

GBR_20050101 (Country and date)

|Column                | Positive impact  | Negative impact |
|:---------------------|:-----------------|:----------------|
|BUS (business)        | +10              |-20              |    
|GOV (government)      | +40              |-15              |
|...                   |                  |                 |

THe numbers in the impact columns represent the relative impact generated by the event according to our impact heuristic function.

The end result of the preprocessed data would look like:

|Date                | USA_BUS_positive  | USA_BUS_negative | GBR_BUS_positive  | GBR_BUS_negative | ... |
|:---------------------|:-----------------|:----------------|:-----------------|:----------------|:----------|
|20050101    | +10.5              |-24.6         | +17.5              |-8.6   | ...      |    
|20050102    | +25.2              |-16.7         | +15.3              |-9.9  | ... |            
|...                   |                 |                 |        |                 |     |

### Impact heuristic function

We try out a few heuristic functions to map categorical variables onto the real line. The general idea is that we are attenuating the intrinsic type of the event by the number of mentions (articles sources etc.) and we hope that it is immediately verifiable.

We normalize the number of articles/mentions/sources by the mean per day, but we can change that timeframe easily

#### First heuristic

$$ \texttt{H_1 = NumArticles(normalized) \times GoldSteinScale} $$

Preliminaries show no linear relationship between regressors and predictors.

#### Second heuristic

$$ \texttt{H_2 = NumArticles(normalized) \times AvgTone} $$

#### Third heuristic

$$ \texttt{H_3 = NumArticles(normalized) \times QuadClass} $$

<center><img src="/assets/H3.PNG" width="100%"></center>

## Classification

### Binary Classification on the Change in Daily Forex Rates
The financial instrument that we look at is the foreign exchange rate between British Pounds (GBP) and American Dollars (USD). We attempt to use the GDELT news data up to day $$ T $$ to infer whether prices will go up or go down between day $$ T - 1 $$ and day $$ T $$. The daily return in exchange rate is calculated as:

$$R_{T-1,T}=ln(\frac{F_{T}}{F_{T-1}})$$

where $$ F_T $$ is the exchange rate of day $$ T $$


We use the GBP-USD exchange rates and the GDELT dataset from 2005. Below is the distribution of daily returns of the exchange rates for 2005:

<center><img src="/assets/week_3/DailyReturnHistogram.png" width="100%"></center>

We treat all returns that exceed 0.5% as positive labels and returns that are below -0.5% as negative labels. This give us 75 positive labels and 90 negative labels which form 165 data points in total. We randomly select 70% of the data points as training data and the
remaining 30% as test data. The experiment is repeated 100 times for each heuristic.

The table below shows the classification accuracy of running kernel SVM with the different heuristics on the test set:

|Heuristic                | Linear Kernel  | Gaussian Kernel |
|:------------------------|:-----------------|:----------------|
|$$ H_1 = NumArticles \times GoldSteinScale $$ | 51.0%   | 58.6%   |    
|$$ H_2 = NumArticles \times AvgTone $$        | 58.4%   | 57.7%   |
|$$ H_3 = NumArticles \times QuadClass $$      | 55.9%   | 61.1%   |

If we reduce the cutoff threshold from 0.5% to 0.3%, the performance of Gaussian Kernel SVM on Heuristic 3 drops to 54%.

The next step is to come up with better heuristics that aggregate multiple metrics over multiple days.

## Markov Switching Vector Autoregressive Models
In a Marjov Switching Model the observed change in a variable between period t and t+1 is assumed to be a random draw from one of two distributions. Which of the two distributions is appropriate depends
 on an unobserved state variable $$s_t$$. When $$s_t=1$$, the observed change $$y_t$$ is draw from $$N(\mu_1,\sigma_1^2)$$ and when $$s_t=2$$, the observed change $$y_t$$ is draw from $$N(\mu_2,\sigma_2^2)$$.
 The state variable is then assumed to evolve according to a Markov Chain such that the probability of being in state 1 at time t given that state 1 obtainted at time t-1 equals $$p_{11}$$. Accordingly, we can 
 build a transition matrix.


### Regressors:

The main Time Series of interest is the log return of the Exchange Rate of GBP to USD.
$$R_{t,t+1}=ln(\frac{F_{t+1}}{F_t})$$
<center><img src="/assets/GBP-USD.PNG" width="100%"></center>
We collected the daily returns (including weekends and holidays) of the exchange rate and multiplied it by 100.
We then evaluated the MSBVAR R package for Markov Switching Bayesian VAR: It computed Maximum Likelihood estimated for an MSVAR(p,h) where p is the number of lags and h is the number of regimes using EM. However, the package is not well-supported and has issues with p>1. Additionally, no exogenous factors have yet been implemented.
Therefore, we sought to define our own MS-VAR by first defining a typical AR(p) with lm() or glm().
For example: VAR(4): Ret[5:350] = Ret[1:346] + Ret[2:347] + Ret[3:348]+ Ret[4:349]
In this case, to include the Exogenous factors, we can simply add them to the forumal with 1-day lag (as indicated by the higher p-values than with 0-lag)
VARX(4): Ret[5:350] ~ Ret[1:346] + Ret[2:347] + Ret[3:348]+ Ret[4:349] + News_data[4:349]

### Model Fit: The Different Heuristics and Variable Selection
<center><img src="/assets/fig1.PNG" width="100%"></center>
The above figure demonstrates the correlation between the vectors of Heuristic 2 (as explained earlier) and major changes in the exchange rate.
We can see that for 2005, the main events (spikes in news) were the London Bombings of July 2007 and Hurricane Katrina (last week of August) and the following damage at the start of September 1st.

Accordingly, we try to fit the separate columns in the heuristic to the time series along with its lagged components using Linear Models or Generalized Linear Models (Gaussian noise).
We then compare the different Heuristics based on the R-Squared, P-Value and most importantly the AIC values.

|Heuristic                |  R-Squared  | P-Value | AIC |
|:------------------------|:-----------------|:----------------|:----------------|
|$$ H_1$$ |  0.2834  |   0.8601  |   531   |
|$$ H_2$$        |  0.1188    |  1.297e-06    |   457.85 |
|$$ H_3$$      | 0.4176  |     0.004931   |  459.51  |

These values are discouraging. R-squared values a bit low which implies a weak/moderate correlation between the predictors and the return time series.
Additionally, $$H_1$$ has a really high P-Value. However, $$H_3$$ has a moderate R-Squared value which indicates a decent correlation in a field as unpredictable as economics.
However, we cannot fully decide on the heuristics until we pick the most import variables from each.
Accordingly, we used Step Forward Regression to figure which subset of columns from each heuristic improves the AIC values.
We ended up with the following new AIC/P-value/R-Square for the following models:
* $$H_3$$: 370 AIC and an R-Squared of 0.2083
* $$H_2$$: 394.94 AIC and an R-Squared of 0.1218
* $$H_1$$: 450 AIC and an R-Squared of 0.1079
$$H_2$$ and $$H_3$$ had really significant p-values which means that even if the R-sqaured value is low, we have statistically significant predictors that can represent the mean change in the response.
<center><img src="/assets/ResH3.PNG" width="100%"></center>

### Model Fit: Number of Regimes
At this point, we decided to go with $$H_2$$ given the great p-value and the potentially moderate R-squared value as well as having the haghest Information Content (from AIC).
We then proceeded to figure the number of regimes that achieves the highest LogLikelihood or AIC.
Initially, we proceeded with the old MSBVAR package on the return time series alone.
We found that 4 regimes had the highest AIC and LogLikelihood. However, we were unable to incorporate our news data into any of the MSBVAR functions.
Therefore, we moved on to use msmFit from the MSwM: Fitting Markov Switching Models package that requires providing a LinearModel (uses EM, see figure for the LikeLihood computation). We then proceeded to evaluate different numbers of regimes that incorporate both the return data and the news data.
It turns out that having 2 regimes had slightly a higher Log-Likehood with 370 in AIC and a 0.2081 in R-Squared.
The transition Probabilities were: 

|          |Regime 1 |Regime 2|
|:---------|:--------|:-------|
|Regime 1  |0.4866221|0.447077|
|Regime 2  |0.5133779|0.552923|

<center><img src="/assets/LL.PNG" width="70%"></center>

Note that the original MSBVAR indicated an interesting subset of regimes for the return data without any lags or any exogenous factors but we couldn't confirm because of issues in the package.
<center><img src="/assets/4Reg.PNG" width="90%"></center>
We can see in this plot the probabiltiies for the 4 different regims in every day 

<center><img src="/assets/4R.PNG" width="90%"></center>
This plot aggregates them in an ugly way where each regime is colored uniquely.
The most likely state for each day is as follows (another ugly plot):
<center><img src="/assets/StateSeq.PNG" width="60%"></center>
We can see that there are 66 days in state 1, 147 in state 2, 79 in state 3 and 67 in state 4.

### Forecasting:
Using the best fitted model, we can predict the next set of Log returns (whether we're going to use 2 regimes or just 1 will depend on the final implementations of the heuristics).
Eventually, the plan is to devise a trading strategy out of the news_data and old returns data. 

